# Import SQLite compatibility module FIRST
try:
    from . import sqlite_compat
except ImportError:
    # Handle case where running as main module
    import sqlite_compat

import yaml
import os
from pathlib import Path
from typing import Dict, Any, List
from langchain_core.prompts import PromptTemplate
from langchain_core.documents import Document
from app.rag_engine import RAGEngine
from app.llm_provider import LLMProvider

class FallbackQAChain:
    """Custom QA Chain for fallback retriever that bypasses Pydantic validation"""
    
    def __init__(self, llm, retriever, prompt):
        self.llm = llm
        self.retriever = retriever
        self.prompt = prompt
    
    def __call__(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Process the QA chain call"""
        query = inputs.get("query", "")
        
        # Retrieve relevant documents
        docs = self.retriever.get_relevant_documents(query)
        
        # Format context from documents
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # Format the prompt
        formatted_prompt = self.prompt.format(context=context, question=query)
        
        # Get LLM response
        response = self.llm.invoke(formatted_prompt)
        
        # Extract text content from response (handle AIMessage objects)
        if hasattr(response, 'content'):
            response_text = response.content
        else:
            response_text = str(response)
        
        return {
            "result": response_text,
            "source_documents": docs
        }
    
    def invoke(self, inputs: Dict[str, Any]) -> Dict[str, Any]:
        """Alternative invoke method"""
        return self.__call__(inputs)

class BabyCareChain:
    def __init__(self, config_path: str = None):
        """åˆå§‹åŒ–è‚²å„¿é¡¾é—®é“¾"""
        if config_path is None:
            # Get the project root directory (parent of app directory)
            project_root = Path(__file__).parent.parent
            config_path = project_root / "config" / "ollama_config.yaml"
        self.config = self._load_config(str(config_path))
        self.llm_provider = None
        self.rag_engine = None
        self.qa_chain = None
        self.custom_prompt = self._load_custom_prompt()
        self._setup_llm()
        
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        with open(config_path, 'r', encoding='utf-8') as file:
            return yaml.safe_load(file)
    
    def _load_custom_prompt(self) -> str:
        """åŠ è½½è‡ªå®šä¹‰æç¤ºè¯"""
        try:
            # Get the project root directory (parent of app directory)
            project_root = Path(__file__).parent.parent
            prompt_path = project_root / "config" / "custom_prompt.txt"
            with open(prompt_path, 'r', encoding='utf-8') as file:
                return file.read().strip()
        except FileNotFoundError:
            return "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„è‚²å„¿é¡¾é—®ï¼Œè¯·ä¸ºç”¨æˆ·æä¾›ä¸“ä¸šã€æ¸©æš–çš„è‚²å„¿å»ºè®®ã€‚"
    
    def _setup_llm(self):
        """è®¾ç½®LLMæä¾›å•†"""
        self.llm_provider = LLMProvider(self.config)
        provider_info = self.llm_provider.get_provider_info()
        print(f"LLM åˆå§‹åŒ–å®Œæˆï¼Œæä¾›å•†: {provider_info['provider']}, æ¨¡å‹: {provider_info['model']}")
    
    def setup_rag_chain(self, data_dirs: List[str], force_rebuild: bool = False):
        """è®¾ç½®RAGé“¾"""
        try:
            # Get the project root directory (parent of app directory)
            project_root = Path(__file__).parent.parent
            config_path = project_root / "config" / "ollama_config.yaml"
            self.rag_engine = RAGEngine(config_path=str(config_path))
            
            if not self.rag_engine.initialize_rag(data_dirs, force_rebuild):
                print("âŒ RAGå¼•æ“åˆå§‹åŒ–å¤±è´¥ï¼Œå°†ä½¿ç”¨ç®€å•æ¨¡å¼")
                self.rag_engine = None
                return False
        except Exception as e:
            print(f"âŒ RAGå¼•æ“åˆå§‹åŒ–å¼‚å¸¸: {str(e)}")
            print("ğŸ’¡ ç³»ç»Ÿå°†åœ¨æ²¡æœ‰çŸ¥è¯†åº“çš„æƒ…å†µä¸‹è¿è¡Œ")
            self.rag_engine = None
            return False
        
        # åˆ›å»ºæç¤ºè¯æ¨¡æ¿
        prompt_template = f"""
{self.custom_prompt}

CRITICAL INSTRUCTIONS:
1. Respond in the SAME LANGUAGE as the user's question
2. Do NOT show any thinking process, <think> tags, or internal reasoning
3. Do NOT mix languages
4. Respond directly and naturally

Based on the following relevant information, answer the user's question:

Relevant information:
{{context}}

User question: {{question}}

Provide detailed, practical advice directly in the SAME LANGUAGE as the user's question (no thinking process):
"""
        
        PROMPT = PromptTemplate(
            template=prompt_template,
            input_variables=["context", "question"]
        )
        
        # åˆ›å»ºQAé“¾ - ç»Ÿä¸€ä½¿ç”¨è‡ªå®šä¹‰QAé“¾é¿å…Pydanticå…¼å®¹æ€§é—®é¢˜
        self.qa_chain = FallbackQAChain(
            llm=self.llm_provider.llm,
            retriever=self.rag_engine.retriever,
            prompt=PROMPT
        )
        
        print("RAGé“¾è®¾ç½®å®Œæˆ")
        return True
    
    def ask_question(self, question: str, baby_info: Dict[str, Any] = None) -> Dict[str, Any]:
        """å¤„ç†ç”¨æˆ·é—®é¢˜"""
        # If RAG is not available, use simple LLM response
        if self.qa_chain is None:
            print("âš ï¸ RAGç³»ç»Ÿä¸å¯ç”¨ï¼Œä½¿ç”¨ç®€å•æ¨¡å¼å›ç­”")
            try:
                # Format question with baby info if available
                enhanced_question = question
                if baby_info:
                    baby_context = self._format_baby_info(baby_info)
                    enhanced_question = f"{baby_context}\n\n{question}"
                
                # Get simple answer without RAG
                answer = self.get_simple_answer(enhanced_question)
                
                return {
                    "answer": answer,
                    "sources": [],
                    "question": question,
                    "baby_info": baby_info,
                    "mode": "simple"
                }
            except Exception as e:
                return {
                    "answer": "æŠ±æ­‰ï¼Œç³»ç»Ÿæš‚æ—¶æ— æ³•å¤„ç†æ‚¨çš„é—®é¢˜ã€‚è¯·ç¨åå†è¯•æˆ–å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿã€‚",
                    "sources": [],
                    "error": str(e),
                    "mode": "error"
                }
        
        try:
            # å¦‚æœæœ‰å®å®ä¿¡æ¯ï¼Œæ·»åŠ åˆ°é—®é¢˜ä¸­
            enhanced_question = question
            if baby_info:
                baby_context = self._format_baby_info(baby_info)
                enhanced_question = f"{baby_context}\n\n{question}"
            
            # è°ƒç”¨QAé“¾
            result = self.qa_chain({"query": enhanced_question})
            
            # æå–æºæ–‡æ¡£ä¿¡æ¯
            sources = []
            if "source_documents" in result:
                for doc in result["source_documents"]:
                    sources.append({
                        "content": doc.page_content[:200] + "..." if len(doc.page_content) > 200 else doc.page_content,
                        "source": doc.metadata.get("source", "æœªçŸ¥æ¥æº")
                    })
            
            return {
                "answer": result["result"],
                "sources": sources,
                "question": question,
                "baby_info": baby_info
            }
            
        except Exception as e:
            print(f"å¤„ç†é—®é¢˜æ—¶å‡ºé”™: {str(e)}")
            return {
                "answer": "æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„é—®é¢˜æ—¶é‡åˆ°äº†ä¸€äº›æŠ€æœ¯é—®é¢˜ã€‚è¯·ç¨åå†è¯•ï¼Œæˆ–è€…å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿã€‚",
                "sources": [],
                "error": str(e)
            }
    
    def _format_baby_info(self, baby_info: Dict[str, Any]) -> str:
        """æ ¼å¼åŒ–å®å®ä¿¡æ¯"""
        info_parts = []
        
        if baby_info.get("age"):
            info_parts.append(f"å®å®å¹´é¾„ï¼š{baby_info['age']}")
        
        if baby_info.get("weight"):
            info_parts.append(f"ä½“é‡ï¼š{baby_info['weight']}")
        
        if baby_info.get("gender"):
            info_parts.append(f"æ€§åˆ«ï¼š{baby_info['gender']}")
        
        if baby_info.get("special_conditions"):
            info_parts.append(f"ç‰¹æ®Šæƒ…å†µï¼š{baby_info['special_conditions']}")
        
        if info_parts:
            return "å®å®ä¿¡æ¯ï¼š" + "ï¼Œ".join(info_parts)
        
        return ""
    
    def get_simple_answer(self, question: str) -> str:
        """è·å–ç®€å•å›ç­”ï¼ˆä¸ä½¿ç”¨RAGï¼‰"""
        try:
            prompt = f"""
{self.custom_prompt}

CRITICAL INSTRUCTIONS:
1. Respond in the SAME LANGUAGE as the user's question
2. Do NOT show any thinking process, <think> tags, or internal reasoning
3. Respond directly and naturally

User question: {question}

Provide professional, warm parenting advice directly in the SAME LANGUAGE as the user's question (no thinking process):
"""
            response = self.llm_provider.invoke(prompt)
            return response
        except Exception as e:
            print(f"è·å–ç®€å•å›ç­”æ—¶å‡ºé”™: {str(e)}")
            return "æŠ±æ­‰ï¼Œæˆ‘ç°åœ¨æ— æ³•å›ç­”æ‚¨çš„é—®é¢˜ã€‚å»ºè®®æ‚¨å’¨è¯¢ä¸“ä¸šåŒ»ç”Ÿæˆ–è‚²å„¿ä¸“å®¶ã€‚"

if __name__ == "__main__":
    # æµ‹è¯•é“¾
    chain = BabyCareChain()
    
    # è®¾ç½®RAGé“¾
    data_dirs = ["data/knowledge", "data/faq"]
    if chain.setup_rag_chain(data_dirs, force_rebuild=True):
        # æµ‹è¯•é—®ç­”
        baby_info = {
            "age": "2ä¸ªæœˆ",
            "weight": "5.5kg",
            "gender": "ç”·"
        }
        
        result = chain.ask_question("å®å®æ™šä¸Šæ€»æ˜¯å“­é—¹ï¼Œæ€ä¹ˆåŠï¼Ÿ", baby_info)
        print(f"é—®é¢˜: {result['question']}")
        print(f"å›ç­”: {result['answer']}")
        print(f"å‚è€ƒæ¥æºæ•°é‡: {len(result['sources'])}")